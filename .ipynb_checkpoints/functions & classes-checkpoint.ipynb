{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2630b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import json\n",
    "import fastparquet\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# drawing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# estimators\n",
    "import xgboost as xgb\n",
    "\n",
    "# functions & classes from neighboring ipynb file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16754c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_df(df3, total_rows =  100000, neg_percent = 50, pos_percent = 50):\n",
    "    df3_pos = df3[df3['flag'] == 1].sample(int(total_rows / 100 * pos_percent))\n",
    "    df3_neg = df3[df3['flag'] == 0].sample(int(total_rows / 100 * neg_percent))\n",
    "    df3_pos = df3_pos.reset_index()\n",
    "    df3_neg = df3_neg.reset_index()\n",
    "    df3_pos = df3_pos.drop('index', axis=1)\n",
    "    df3_neg = df3_neg.drop('index', axis=1)\n",
    "    df3 = pd.concat([df3_pos, df3_neg])\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3229c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf36a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overdue_and_quick_closure_duck(df_time_exp):\n",
    "    \n",
    "#     print('overdue started')\n",
    "    con = duckdb.connect(':memory:')\n",
    "    \n",
    "    # new column, product-wise\n",
    "    df_time_exp['closed_faster_than_expected'] = df_time_exp['pre_fterm'] < df_time_exp['pre_pterm'] \n",
    "    df_time_exp['closed_faster_than_expected'] = df_time_exp['closed_faster_than_expected'].apply(lambda x: int(x))\n",
    "\n",
    "    # new column, product-wise\n",
    "    df_time_exp['overdue_severity'] = df_time_exp['is_zero_loans5'] + df_time_exp['is_zero_loans530']*2 + df_time_exp['is_zero_loans3060']*3 + df_time_exp['is_zero_loans6090']*4 + df_time_exp['is_zero_loans90']*5\n",
    "    \n",
    "    # two new columns, client-wise, ready for aggregation\n",
    "    result = con.execute('''\n",
    "    SELECT id, \n",
    "    cast(SUM(overdue_severity) as float) / max(rn) AS prone_to_overdue, \n",
    "    cast(SUM(closed_faster_than_expected) as float) / max(rn) as prone_to_close_faster\n",
    "    FROM df_time_exp\n",
    "    GROUP BY id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','prone_to_overdue', 'prone_to_close_faster'])\n",
    "    df_time_exp = pd.merge(result, df_time_exp, on='id', how='left')\n",
    "    \n",
    "#     print('overdue ended')\n",
    "    return df_time_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eccc6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overdue_and_quick_closure_duck_optimised(df):\n",
    "    # two new columns, product-wise\n",
    "#     df = df.sort_values('id')\n",
    "#     df = df.reset_index()\n",
    "#     df = df.drop('index', axis=1)\n",
    "    #     print('ovedue started')\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    result = con.execute('''\n",
    "\n",
    "\n",
    "    SELECT  id, \n",
    "            pre_fterm, \n",
    "            pre_pterm,\n",
    "               CASE \n",
    "                   WHEN pre_fterm < pre_pterm THEN 1 \n",
    "                   ELSE 0 \n",
    "               END AS closed_faster_than_expected,\n",
    "            is_zero_loans5 + is_zero_loans530*2 + is_zero_loans3060*3 + is_zero_loans6090*4 + is_zero_loans90*5 as overdue_severity,\n",
    "    FROM df\n",
    "\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','pre_fterm', 'pre_pterm', 'closed_faster_than_expected', 'overdue_severity'])\n",
    "\n",
    "    # result = result.drop('id', axis=1)\n",
    "    # result\n",
    "\n",
    "\n",
    "    for col in result.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result[col]\n",
    "    # df\n",
    "\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    # two new columns, client-wise, ready for aggregation\n",
    "    result = con.execute('''\n",
    "    SELECT id, \n",
    "    cast(SUM(overdue_severity) as float) / max(rn) AS prone_to_overdue, \n",
    "    cast(SUM(closed_faster_than_expected) as float) / max(rn) as prone_to_close_faster\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','prone_to_overdue', 'prone_to_close_faster'])\n",
    "    # result\n",
    "\n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.prone_to_overdue, result.prone_to_close_faster\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','prone_to_overdue', 'prone_to_close_faster'])\n",
    "    result2\n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('overdue ended')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab42f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_status_prevalent_atm(df):\n",
    "    # new column, client-wise, ready for aggregation\n",
    "#     print('credit status started')\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id, \n",
    "    mode(enc_loans_credit_status) as credit_status_prevalent_atm, \n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id', 'credit_status_prevalent_atm'])\n",
    "    \n",
    "\n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.credit_status_prevalent_atm\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','credit_status_prevalent_atm'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('credit status ended')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2691b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_status_general_mean(df):\n",
    "    # new column, client-wise, ready for aggregation\n",
    "#     print('general cs start')\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    cast(sum(enc_loans_credit_status) as float) / max(rn) as credit_status_general_mean\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','credit_status_general_mean'])\n",
    "    \n",
    "\n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.credit_status_general_mean\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','credit_status_general_mean'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('general cs end')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6dfb221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_line_length(df):\n",
    "    # new column, client-wise, ready for aggregation\n",
    "#     print('cs length start')\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    count(rn) as credit_line_length\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','credit_line_length'])\n",
    "    \n",
    "\n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.credit_line_length\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','credit_line_length'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('cs length end')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ccceee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_auto(df):\n",
    "    # кодирование нужных колонок\n",
    "    \n",
    "    cols_to_encode = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == 'id':\n",
    "            continue\n",
    "        if col == 'flag':\n",
    "            continue\n",
    "        value = len(df[col].unique())\n",
    "\n",
    "        if value == 2:\n",
    "            continue\n",
    "        if 3 <= value and value < 10:\n",
    "            cols_to_encode.append(col)\n",
    "        if value > 10:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    df_ohe = pd.DataFrame.from_records(ohe.fit_transform(df[cols_to_encode]), columns=ohe.get_feature_names_out())\n",
    "    df = pd.concat([df, df_ohe], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def ohe_handpicked(df):\n",
    "    # кодирование нужных колонок\n",
    "#     print('ohe starting')\n",
    "    cols_to_encode = [\n",
    "        'pre_loans_outstanding',\n",
    "        'pre_loans_max_overdue_sum',\n",
    "#         'pre_loans_credit_cost_rate',\n",
    "#         'pre_util',\n",
    "#         'pre_over2limit',\n",
    "#         'pre_maxover2limit',\n",
    "        'enc_loans_account_holder_type',\n",
    "        'enc_loans_account_cur',\n",
    "        'enc_loans_credit_status',   \n",
    "        'enc_loans_credit_type',\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "    df_ohe = pd.DataFrame.from_records(ohe.fit_transform(df[cols_to_encode]), columns=ohe.get_feature_names_out())\n",
    "    df = pd.concat([df, df_ohe], axis=1)\n",
    "#     print('ohe ended')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35bc136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropping_cols(df):\n",
    "#     print('dropping start')\n",
    "    cols_to_drop = ['enc_paym_0', 'enc_paym_1', 'enc_paym_2', 'enc_paym_3', 'enc_paym_4', 'enc_paym_5', 'enc_paym_6', 'enc_paym_7',\n",
    "    'enc_paym_8', 'enc_paym_9', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12', 'enc_paym_13', 'enc_paym_14', 'enc_paym_15',\n",
    "    'enc_paym_16', 'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_20', 'enc_paym_21', 'enc_paym_22',\n",
    "    'enc_paym_23', 'enc_paym_24']\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "#     print('dropping end')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f33b8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_func(df):\n",
    "    try:\n",
    "        answers = pd.read_csv('train_target.csv')\n",
    "    except FileNotFoundError:\n",
    "        answers = pd.read_csv('train_data/train_target.csv')\n",
    "    return pd.merge(df, answers, on='id', how='left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d4031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_func_optimised(df):\n",
    "#     print('answers start')\n",
    "\n",
    "    try:\n",
    "        answers = pd.read_csv('train_target.csv')\n",
    "    except FileNotFoundError:\n",
    "        answers = pd.read_csv('train_data/train_target.csv')\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    result = con.execute('''\n",
    "    SELECT df.id, answers.flag\n",
    "    FROM df join answers on df.id = answers.id \n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','flag'])\n",
    "    \n",
    "    for col in result.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result[col]\n",
    "        \n",
    "#     print('answers end')\n",
    "    return df\n",
    "\n",
    "def answers_func_optimised_for_pipe(df_og):\n",
    "#     print('answers start')\n",
    "    df = df_og.copy()\n",
    "    try:\n",
    "        answers = pd.read_csv('train_target.csv')\n",
    "    except FileNotFoundError:\n",
    "        answers = pd.read_csv('train_data/train_target.csv')\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    result = con.execute('''\n",
    "    SELECT df.id, answers.flag\n",
    "    FROM df join answers on df.id = answers.id \n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','flag'])\n",
    "    \n",
    "    for col in result.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result[col]\n",
    "#     print('answers end')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2523d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_dataset_from_local(path_to_dataset: str = 'train_data/', start_from: int = 0,\n",
    "                                     num_parts_to_read: int = 2, columns=None, verbose=True,\n",
    "                                   transformers: list = [],\n",
    "                                   ):\n",
    "    \"\"\"\n",
    "    Предоставленный инструмент по сбору итогового датасета.\n",
    "    Доработки:\n",
    "    1) теперь на вход так же принимает лист функций, в дальнейшем они оборачиваются в function_transformer \n",
    "    и служат этапами в пайплайне по обработке данных\n",
    "    \n",
    "    2) была заменена библиотека для чтения паркет-файлов - pandas и pyarrow моментально \n",
    "    убивают ядро - вместо этого используется fastparquet - свою функцию выполняет.\n",
    "    \n",
    "    читает num_parts_to_read партиций, преобразовывает их к pd.DataFrame и возвращает\n",
    "    :param path_to_dataset: путь до директории с партициями\n",
    "    :param start_from: номер партиции, с которой нужно начать чтение\n",
    "    :param num_parts_to_read: количество партиций, которые требуется прочитать\n",
    "    :param columns: список колонок, которые нужно прочитать из партиции\n",
    "    :return: pd.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                              if filename.startswith('train')])\n",
    "    print(dataset_paths)\n",
    "\n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "    if verbose:\n",
    "        print('Reading chunks:\\n')\n",
    "        for chunk in chunks:\n",
    "            print(chunk)\n",
    "            \n",
    "            \n",
    "    transformers_trf = []  \n",
    "    for i in range(len(transformers)):  \n",
    "            transformers_trf.append((transformers[i].__name__+'_trf', FunctionTransformer(transformers[i])))  # look for error here!!!!      \n",
    "    pipeline = Pipeline(transformers_trf) \n",
    "    \n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"): \n",
    "#     for chunk_path in tqdm.notebook.tqdm(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        print('chunk_path', chunk_path)\n",
    "        chunk = fastparquet.ParquetFile(chunk_path).to_pandas()\n",
    "        \n",
    "        chunk = pipeline.fit_transform(chunk)  # look for error here!!!!      \n",
    "        \n",
    "        res.append(chunk)\n",
    "        \n",
    "        \n",
    "    res = pd.concat(res).reset_index(drop=True)\n",
    "    \n",
    "    keys = res.isnull().sum().keys()\n",
    "    nan_values = res.isnull().sum()\n",
    "    cols_w_nans = []\n",
    "    for key in keys:\n",
    "        if nan_values[key] > 0:\n",
    "            cols_w_nans.append(key)\n",
    "\n",
    "    for col in cols_w_nans:\n",
    "        res.loc[res[col].isna() == True, col] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "147a2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_dataset_from_local_og(path_to_dataset: str, start_from: int = 0,\n",
    "                                     num_parts_to_read: int = 2, columns=None, verbose=True,\n",
    "                                   ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Предоставленный инструмент по сбору итогового датасета.\n",
    "    Доработки:\n",
    "    1) была заменена библиотека для чтения паркет-файлов - pandas и pyarrow моментально \n",
    "    убивают ядро - вместо этого используется fastparquet - свою функцию выполняет.\n",
    "    \n",
    "    читает num_parts_to_read партиций, преобразовывает их к pd.DataFrame и возвращает\n",
    "    :param path_to_dataset: путь до директории с партициями\n",
    "    :param start_from: номер партиции, с которой нужно начать чтение\n",
    "    :param num_parts_to_read: количество партиций, которые требуется прочитать\n",
    "    :param columns: список колонок, которые нужно прочитать из партиции\n",
    "    :return: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    res = []\n",
    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                              if filename.startswith('train')])\n",
    "    print(dataset_paths)\n",
    "\n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "    if verbose:\n",
    "        print('Reading chunks:\\n')\n",
    "        for chunk in chunks:\n",
    "            print(chunk)\n",
    "            \n",
    "    \n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"): \n",
    "#     for chunk_path in tqdm.notebook.tqdm(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        print('chunk_path', chunk_path)\n",
    "        chunk = fastparquet.ParquetFile(chunk_path).to_pandas()\n",
    "        \n",
    "        \n",
    "        res.append(chunk)\n",
    "        \n",
    "        \n",
    "    res = pd.concat(res).reset_index(drop=True)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d28f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_cols_for_sql_2(df) -> str:    \n",
    "    tst = str([f't1.{col}' for col in  df.columns]).replace(\"'\",'').replace('\\n','').replace(' ','').replace('Index([','').replace(']','').replace(',dtype=object)','').replace('[','')\n",
    "    return tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c0da24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_rn(df):\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT {str_cols_for_sql_2(df)}\n",
    "    FROM df t1\n",
    "    JOIN (\n",
    "        SELECT id, MAX(rn) AS max_rn\n",
    "        FROM df\n",
    "        GROUP BY id\n",
    "    ) t2 ON t1.id = t2.id AND t1.rn = t2.max_rn;\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=[col for col in df.columns])\n",
    "    \n",
    "    result = result.sort_values(by='id')\n",
    "    result = result.reset_index(drop=True)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e65385ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_imputer(res):\n",
    "#     print('zero_imputer start')\n",
    "    keys = res.isnull().sum().keys()\n",
    "    nan_values = res.isnull().sum()\n",
    "    cols_w_nans = []\n",
    "    for key in keys:\n",
    "        if nan_values[key] > 0:\n",
    "            cols_w_nans.append(key)\n",
    "\n",
    "    for col in cols_w_nans:\n",
    "        res.loc[res[col].isna() == True, col] = 0        \n",
    "#     print('zero_imputer end')   \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54d5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_agregating(df):\n",
    "    \n",
    "#     print('custom agg start')\n",
    "    data  = df.id.unique()\n",
    "    tst_df = pd.DataFrame.from_records([])\n",
    "    tst_df['id'] = data\n",
    "    \n",
    "    old_cols = [\n",
    "    'pre_since_opened',\n",
    "    'pre_since_confirmed',\n",
    "    'pre_loans_outstanding',\n",
    "    'pre_loans_total_overdue',\n",
    "    'pre_loans_max_overdue_sum',\n",
    "    'pre_loans_credit_cost_rate',\n",
    "    'pre_loans5',\n",
    "    'pre_loans530',\n",
    "    'pre_loans3060',\n",
    "    'pre_loans6090',\n",
    "    'pre_loans90',\n",
    "    'pre_util',\n",
    "    'pre_over2limit',\n",
    "    'pre_maxover2limit',\n",
    "    'is_zero_util',\n",
    "    'is_zero_over2limit',\n",
    "    'is_zero_maxover2limit',\n",
    "    'enc_loans_account_holder_type',\n",
    "    'enc_loans_credit_status',\n",
    "    'enc_loans_credit_type',\n",
    "    'enc_loans_account_cur',\n",
    "    'pclose_flag',\n",
    "    'fclose_flag',\n",
    "    ]\n",
    "    \n",
    "    for old_col in old_cols:\n",
    "        \n",
    "        \n",
    "        \n",
    "        temp = df[old_col].value_counts().keys() #[df.id == 2500000]\n",
    "        new_cols = []\n",
    "        for entry in temp:\n",
    "            new_cols.append(old_col+'_'+str(entry).replace('.0',''))\n",
    "        \n",
    "    \n",
    "        for col in new_cols:\n",
    "            number = col[-2:].replace('_','')\n",
    "\n",
    "\n",
    "\n",
    "            con = duckdb.connect(':memory:')\n",
    "\n",
    "            con.register('df', df)\n",
    "            result = con.execute(f'''\n",
    "\n",
    "\n",
    "            select id, count({old_col}) as {col}\n",
    "            FROM df\n",
    "            where {old_col} = {number}\n",
    "            group by id\n",
    "\n",
    "\n",
    "            '''\n",
    "            ).fetchall()\n",
    "\n",
    "\n",
    "            result = pd.DataFrame.from_records(result, columns=['id', f'{old_col}_{number}'])\n",
    "            tst_df = pd.merge(tst_df, result, on='id', how='left')\n",
    "            \n",
    "    \n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT {str_cols_for_sql_2(df)}\n",
    "    FROM df t1\n",
    "    JOIN (\n",
    "        SELECT id, MAX(rn) AS max_rn\n",
    "        FROM df\n",
    "        GROUP BY id\n",
    "    ) t2 ON t1.id = t2.id AND t1.rn = t2.max_rn;\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=[col for col in df.columns])\n",
    "    \n",
    "    result = result.sort_values(by='id')\n",
    "    result = result.reset_index(drop=True)\n",
    "    \n",
    "    df_result = pd.merge(tst_df, result, on='id', how='left')\n",
    "    \n",
    "#     print('custom agg end')\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62ae4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_agregating_optimised(df):\n",
    "#     print('custom agg start')\n",
    "    data  = df.id.unique()\n",
    "    tst_df = pd.DataFrame.from_records([])\n",
    "    tst_df['id'] = data\n",
    "    \n",
    "    old_cols = [\n",
    "    'pre_since_opened',\n",
    "    'pre_since_confirmed',\n",
    "    'pre_loans_outstanding',\n",
    "    'pre_loans_total_overdue',\n",
    "    'pre_loans_max_overdue_sum',\n",
    "    'pre_loans_credit_cost_rate',\n",
    "    'pre_loans5',\n",
    "    'pre_loans530',\n",
    "    'pre_loans3060',\n",
    "    'pre_loans6090',\n",
    "    'pre_loans90',\n",
    "    'pre_util',\n",
    "    'pre_over2limit',\n",
    "    'pre_maxover2limit',\n",
    "    'is_zero_util',\n",
    "    'is_zero_over2limit',\n",
    "    'is_zero_maxover2limit',\n",
    "    'enc_loans_account_holder_type',\n",
    "    'enc_loans_credit_status',\n",
    "    'enc_loans_credit_type',\n",
    "    'enc_loans_account_cur',\n",
    "    'pclose_flag',\n",
    "    'fclose_flag',\n",
    "    ]\n",
    "    \n",
    "    for old_col in old_cols:\n",
    "        \n",
    "        \n",
    "        \n",
    "        temp = df[old_col].value_counts().keys() #[df.id == 2500000]\n",
    "        new_cols = []\n",
    "        for entry in temp:\n",
    "            new_cols.append(old_col+'_'+str(entry).replace('.0',''))\n",
    "        \n",
    "        for col in new_cols:\n",
    "#             print(col)\n",
    "            number = col[-2:].replace('_','')\n",
    "\n",
    "\n",
    "\n",
    "            con = duckdb.connect(':memory:')\n",
    "\n",
    "            result = con.execute(f'''\n",
    "\n",
    "\n",
    "            select id, count({old_col}) as {col}\n",
    "            FROM df\n",
    "            where {old_col} = {number}\n",
    "            group by id\n",
    "\n",
    "\n",
    "            '''\n",
    "            ).fetchall()\n",
    "\n",
    "\n",
    "            result = pd.DataFrame.from_records(result, columns=['id', f'{old_col}_{number}_agg'])\n",
    "            \n",
    "            result2 = con.execute(f'''\n",
    "            SELECT tst_df.id, result.{old_col}_{number}_agg\n",
    "            FROM tst_df left join result on tst_df.id = result.id\n",
    "            order by tst_df.id\n",
    "            '''\n",
    "                                ).fetchall()\n",
    "            result2 = pd.DataFrame.from_records(result2, columns=['id',f'{old_col}_{number}_agg'])\n",
    "\n",
    "            for col in result2.columns:\n",
    "                if col =='id':\n",
    "                    continue\n",
    "                tst_df[col] = result2[col]\n",
    "    \n",
    "    tst_df = zero_imputer(tst_df)\n",
    "        \n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT {str_cols_for_sql_2(df)}\n",
    "    FROM df t1\n",
    "    JOIN (\n",
    "        SELECT id, MAX(rn) AS max_rn\n",
    "        FROM df\n",
    "        GROUP BY id\n",
    "    ) t2 ON t1.id = t2.id AND t1.rn = t2.max_rn;\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=[col for col in df.columns])\n",
    "    \n",
    "    result = result.sort_values(by='id')\n",
    "    result = result.reset_index(drop=True)\n",
    "    \n",
    "#     df_result = pd.merge(tst_df, result, on='id', how='left')\n",
    "    for col in result.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        tst_df[col] = result[col]\n",
    "        \n",
    "#     print('custom agg end')   \n",
    "    return tst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30f2bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_agregating_optimised_for_pipe(df):\n",
    "#     print('custom agg start')\n",
    "    data  = df.id.unique()\n",
    "    tst_df = pd.DataFrame.from_records([])\n",
    "    tst_df['id'] = data\n",
    "    \n",
    "    old_cols = [\n",
    "    'pre_since_opened',\n",
    "    'pre_since_confirmed',\n",
    "    'pre_loans_outstanding',\n",
    "    'pre_loans_total_overdue',\n",
    "    'pre_loans_max_overdue_sum',\n",
    "    'pre_loans_credit_cost_rate',\n",
    "    'pre_loans5',\n",
    "    'pre_loans530',\n",
    "    'pre_loans3060',\n",
    "    'pre_loans6090',\n",
    "    'pre_loans90',\n",
    "    'pre_util',\n",
    "    'pre_over2limit',\n",
    "    'pre_maxover2limit',\n",
    "    'is_zero_util',\n",
    "    'is_zero_over2limit',\n",
    "    'is_zero_maxover2limit',\n",
    "    'enc_loans_account_holder_type',\n",
    "    'enc_loans_credit_status',\n",
    "    'enc_loans_credit_type',\n",
    "    'enc_loans_account_cur',\n",
    "    'pclose_flag',\n",
    "    'fclose_flag',\n",
    "    ]\n",
    "    \n",
    "    for old_col in old_cols:\n",
    "        \n",
    "        \n",
    "        \n",
    "        temp = df[old_col].value_counts().keys() #[df.id == 2500000]\n",
    "        new_cols = []\n",
    "        for entry in temp:\n",
    "            new_cols.append(old_col+'_'+str(entry).replace('.0',''))\n",
    "        \n",
    "        for col in new_cols:\n",
    "#             print(col)\n",
    "            number = col[-2:].replace('_','')\n",
    "\n",
    "\n",
    "\n",
    "            con = duckdb.connect(':memory:')\n",
    "\n",
    "            result = con.execute(f'''\n",
    "\n",
    "\n",
    "            select id, count({old_col}) as {col}\n",
    "            FROM df\n",
    "            where {old_col} = {number}\n",
    "            group by id\n",
    "\n",
    "\n",
    "            '''\n",
    "            ).fetchall()\n",
    "\n",
    "\n",
    "            result = pd.DataFrame.from_records(result, columns=['id', f'{old_col}_{number}_agg'])\n",
    "            \n",
    "            result2 = con.execute(f'''\n",
    "            SELECT tst_df.id, result.{old_col}_{number}_agg\n",
    "            FROM tst_df left join result on tst_df.id = result.id\n",
    "            order by tst_df.id\n",
    "            '''\n",
    "                                ).fetchall()\n",
    "            result2 = pd.DataFrame.from_records(result2, columns=['id',f'{old_col}_{number}_agg'])\n",
    "\n",
    "            for col in result2.columns:\n",
    "                if col =='id':\n",
    "                    continue\n",
    "                tst_df[col] = result2[col]\n",
    "    \n",
    "    tst_df = zero_imputer(tst_df)\n",
    "        \n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT {str_cols_for_sql_2(df)}\n",
    "    FROM df t1\n",
    "    JOIN (\n",
    "        SELECT id, MAX(rn) AS max_rn\n",
    "        FROM df\n",
    "        GROUP BY id\n",
    "    ) t2 ON t1.id = t2.id AND t1.rn = t2.max_rn;\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=[col for col in df.columns])\n",
    "    \n",
    "    result = result.sort_values(by='id')\n",
    "    result = result.reset_index(drop=True)\n",
    "    \n",
    "#     df_result = pd.merge(tst_df, result, on='id', how='left')\n",
    "    for col in result.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        tst_df[col] = result[col]\n",
    "        \n",
    "#     print('custom agg end')  \n",
    "    \n",
    "    return tst_df.drop('flag', axis=1), tst_df['flag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "708227ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outstanding_loans_summ(df):\n",
    "#     print('outstanding_loans_summ start')\n",
    "    # сумма неоплаченных долгов по займам\n",
    "    \n",
    "    df = df.sort_values('id')\n",
    "    df = df.reset_index()\n",
    "    df = df.drop('index', axis=1)\n",
    "    \n",
    "    \n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    sum(pre_loans_outstanding) as outstanding_loans_summ\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','outstanding_loans_summ'])\n",
    "\n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.outstanding_loans_summ\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','outstanding_loans_summ'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "    \n",
    "#     print('outstanding_loans_summ end')     \n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def outstanding_loans_summ_sq(df):\n",
    "    \n",
    "    # сумма неоплаченных долгов по займам\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    power(sum(pre_loans_outstanding), 2) as outstanding_loans_summ_sq\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','outstanding_loans_summ_sq'])\n",
    "    \n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.outstanding_loans_summ_sq\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','outstanding_loans_summ_sq'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('outstanding_loans_summ_sq end')     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4b438d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loans_overdue_summ(df):\n",
    "#     print('loans_overdue_summ start')\n",
    "    # сумма неоплаченных долгов по займам\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    sum(pre_loans_total_overdue) as loans_overdue_summ\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','loans_overdue_summ'])\n",
    "    \n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.loans_overdue_summ\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','loans_overdue_summ'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('loans_overdue_summ end')    \n",
    "    return df\n",
    "\n",
    "\n",
    "def loans_overdue_summ_sq(df):\n",
    "#     print('loans_overdue_summ start')\n",
    "    # сумма неоплаченных долгов по займам в квадрате\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    power(sum(pre_loans_total_overdue), 2) as loans_overdue_summ_sq\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','loans_overdue_summ_sq'])\n",
    "\n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.loans_overdue_summ_sq\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','loans_overdue_summ_sq'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('loans_overdue_summ_sq end')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99729de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_overdue(df):\n",
    "#     print('max_overdue start')\n",
    "    # максимальная просрочка за всю кредитную историю \n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    max(pre_loans_max_overdue_sum) as max_overdue\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','max_overdue'])\n",
    "    \n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.max_overdue\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','max_overdue'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('max_overdue end')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "866322e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_limit(df):\n",
    "#     print('credit_limit start')\n",
    "    # максимальная кредитный лимит за всю кредитную историю\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    max(pre_loans_credit_limit) as credit_limit\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','credit_limit'])\n",
    "    \n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.credit_limit\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','credit_limit'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('credit_limit end')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a416f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_costs(df):\n",
    "#     print('credit_costs start')\n",
    "    # сумма дохода от пользования кредитными продуктами\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    sum(pre_loans_credit_cost_rate) as credit_costs\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','credit_costs'])\n",
    "    \n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.credit_costs\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','credit_costs'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('credit_costs end')\n",
    "    return df\n",
    "\n",
    "def credit_costs_sq(df):\n",
    "#     print('credit_costs_sq start')\n",
    "    # сумма дохода от пользования кредитными продуктами в квадрате\n",
    "    con = duckdb.connect(':memory:')\n",
    "\n",
    "    con.register('df', df)\n",
    "\n",
    "    result = con.execute(f'''\n",
    "    SELECT id,  \n",
    "    power(sum(pre_loans_credit_cost_rate), 2) as credit_costs_sq\n",
    "    FROM df\n",
    "    GROUP BY id\n",
    "    '''\n",
    "    ).fetchall()\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_records(result, columns=['id','credit_costs_sq'])\n",
    "    \n",
    "    result2 = con.execute('''\n",
    "    SELECT df.id, result.credit_costs_sq\n",
    "    FROM df join result on df.id = result.id\n",
    "    order by df.id\n",
    "    '''\n",
    "                        ).fetchall()\n",
    "\n",
    "    result2 = pd.DataFrame.from_records(result2, columns=['id','credit_costs_sq'])\n",
    "    \n",
    "    for col in result2.columns:\n",
    "        if col =='id':\n",
    "            continue\n",
    "        df[col] = result2[col]\n",
    "#     print('credit_costs_sq end')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08f08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60aff6df",
   "metadata": {},
   "source": [
    "default_transformers = [answers_func_optimised_for_pipe,\n",
    "             outstanding_loans_summ,\n",
    "             outstanding_loans_summ_sq,\n",
    "             loans_overdue_summ,\n",
    "             loans_overdue_summ_sq,\n",
    "             credit_limit,\n",
    "             credit_costs,\n",
    "             credit_costs_sq,\n",
    "             max_overdue,\n",
    "             credit_status_general_mean,\n",
    "             credit_status_prevalent_atm,\n",
    "             overdue_and_quick_closure_duck_optimised,\n",
    "             credit_line_length,\n",
    "             ohe_handpicked,\n",
    "             dropping_cols,\n",
    "             custom_agregating_optimised,\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1446e",
   "metadata": {},
   "source": [
    "def transform(df,\n",
    "                 transformers: list = default_transformers,\n",
    "                ) -> pd.DataFrame:\n",
    "\n",
    "\n",
    "  \n",
    "            \n",
    "                \n",
    "    transformers_trf = []  \n",
    "    for i in range(len(transformers)):  \n",
    "            transformers_trf.append((transformers[i].__name__+'_trf', FunctionTransformer(transformers[i])))  # look for error here!!!!      \n",
    "    pipeline = Pipeline(transformers_trf) \n",
    "    \n",
    "\n",
    "    res = pipeline.fit_transform(df)  # look for error here!!!!      \n",
    "\n",
    "        \n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dc7735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_checker(df):\n",
    "\n",
    "    try:\n",
    "        with open('all_columns_list.txt', 'r') as file:\n",
    "            all_columns = [line.strip() for line in file.readlines()]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        all_columns = [\n",
    "            'closed_faster_than_expected',     'credit_costs',     'credit_costs_sq',     'credit_limit',     'credit_line_length',     'credit_status_general_mean',     'credit_status_prevalent_atm',     'enc_loans_account_cur',     'enc_loans_account_cur_0',     'enc_loans_account_cur_0_agg',     'enc_loans_account_cur_1',     'enc_loans_account_cur_1_agg',     'enc_loans_account_cur_2',     'enc_loans_account_cur_2_agg',     'enc_loans_account_cur_3',     'enc_loans_account_cur_3_agg',     'enc_loans_account_holder_type',     'enc_loans_account_holder_type_0',     'enc_loans_account_holder_type_0_agg',     'enc_loans_account_holder_type_1',     'enc_loans_account_holder_type_1_agg',     'enc_loans_account_holder_type_2',     'enc_loans_account_holder_type_2_agg',\n",
    "         'enc_loans_account_holder_type_3',     'enc_loans_account_holder_type_3_agg',     'enc_loans_account_holder_type_4',     'enc_loans_account_holder_type_4_agg',     'enc_loans_account_holder_type_5',     'enc_loans_account_holder_type_5_agg',     'enc_loans_account_holder_type_6',     'enc_loans_account_holder_type_6_agg',     'enc_loans_credit_status',     'enc_loans_credit_status_0',     'enc_loans_credit_status_0_agg',     'enc_loans_credit_status_1',     'enc_loans_credit_status_1_agg',     'enc_loans_credit_status_2',     'enc_loans_credit_status_2_agg',     'enc_loans_credit_status_3',     'enc_loans_credit_status_3_agg',     'enc_loans_credit_status_4',     'enc_loans_credit_status_4_agg',     'enc_loans_credit_status_5',     'enc_loans_credit_status_5_agg',     'enc_loans_credit_status_6',     'enc_loans_credit_status_6_agg',     'enc_loans_credit_type',     'enc_loans_credit_type_0',\n",
    "         'enc_loans_credit_type_0_agg',    'enc_loans_credit_type_1',     'enc_loans_credit_type_1_agg',     'enc_loans_credit_type_2',     'enc_loans_credit_type_2_agg',     'enc_loans_credit_type_3',     'enc_loans_credit_type_3_agg',     'enc_loans_credit_type_4',     'enc_loans_credit_type_4_agg',     'enc_loans_credit_type_5',     'enc_loans_credit_type_5_agg',     'enc_loans_credit_type_6',     'enc_loans_credit_type_6_agg',     'enc_loans_credit_type_7',     'enc_loans_credit_type_7_agg',     'fclose_flag',     'fclose_flag_0_agg',     'fclose_flag_1_agg',     'flag',     'id',     'is_zero_loans3060',     'is_zero_loans5',     'is_zero_loans530',     'is_zero_loans6090',     'is_zero_loans90',     'is_zero_maxover2limit',\n",
    "         'is_zero_maxover2limit_0_agg',     'is_zero_maxover2limit_1_agg',     'is_zero_over2limit',     'is_zero_over2limit_0_agg',     'is_zero_over2limit_1_agg',     'is_zero_util',     'is_zero_util_0_agg',     'is_zero_util_1_agg',     'loans_overdue_summ',     'loans_overdue_summ_sq',     'max_overdue',     'outstanding_loans_summ',     'outstanding_loans_summ_sq',     'overdue_severity',     'pclose_flag',     'pclose_flag_0_agg',     'pclose_flag_1_agg',     'pre_fterm',     'pre_loans3060',     'pre_loans3060_0_agg',     'pre_loans3060_1_agg',     'pre_loans3060_2_agg',     'pre_loans3060_3_agg',     'pre_loans3060_4_agg',     'pre_loans3060_5_agg',     'pre_loans3060_6_agg',     'pre_loans3060_7_agg',\n",
    "         'pre_loans3060_8_agg',     'pre_loans3060_9_agg',     'pre_loans5',     'pre_loans530',     'pre_loans530_0_agg',     'pre_loans530_10_agg',     'pre_loans530_11_agg',     'pre_loans530_12_agg',     'pre_loans530_13_agg',     'pre_loans530_14_agg',     'pre_loans530_15_agg',     'pre_loans530_16_agg',     'pre_loans530_17_agg',     'pre_loans530_18_agg',     'pre_loans530_19_agg',     'pre_loans530_1_agg',     'pre_loans530_2_agg',     'pre_loans530_3_agg',     'pre_loans530_4_agg',     'pre_loans530_5_agg',     'pre_loans530_6_agg',     'pre_loans530_7_agg',     'pre_loans530_8_agg',     'pre_loans530_9_agg',     'pre_loans5_0_agg',     'pre_loans5_10_agg',     'pre_loans5_11_agg',     'pre_loans5_13_agg',     'pre_loans5_16_agg',     'pre_loans5_1_agg',\n",
    "         'pre_loans5_2_agg',     'pre_loans5_3_agg',     'pre_loans5_5_agg',     'pre_loans5_6_agg',     'pre_loans5_7_agg',     'pre_loans5_8_agg',     'pre_loans5_9_agg',     'pre_loans6090',     'pre_loans6090_0_agg',     'pre_loans6090_1_agg',     'pre_loans6090_2_agg',     'pre_loans6090_3_agg',     'pre_loans6090_4_agg',     'pre_loans90',     'pre_loans90_10_agg',     'pre_loans90_13_agg',     'pre_loans90_14_agg',     'pre_loans90_19_agg',     'pre_loans90_2_agg',     'pre_loans90_3_agg',     'pre_loans90_8_agg',     'pre_loans_credit_cost_rate',     'pre_loans_credit_cost_rate_0_agg',     'pre_loans_credit_cost_rate_10_agg',     'pre_loans_credit_cost_rate_11_agg',     'pre_loans_credit_cost_rate_12_agg',     'pre_loans_credit_cost_rate_13_agg',     'pre_loans_credit_cost_rate_1_agg',     'pre_loans_credit_cost_rate_2_agg',     'pre_loans_credit_cost_rate_3_agg',     'pre_loans_credit_cost_rate_4_agg',     'pre_loans_credit_cost_rate_5_agg',     'pre_loans_credit_cost_rate_6_agg',     'pre_loans_credit_cost_rate_7_agg',\n",
    "         'pre_loans_credit_cost_rate_8_agg',     'pre_loans_credit_cost_rate_9_agg',     'pre_loans_credit_limit',     'pre_loans_max_overdue_sum',     'pre_loans_max_overdue_sum_0',     'pre_loans_max_overdue_sum_0_agg',     'pre_loans_max_overdue_sum_1',     'pre_loans_max_overdue_sum_1_agg',     'pre_loans_max_overdue_sum_2',     'pre_loans_max_overdue_sum_2_agg',     'pre_loans_max_overdue_sum_3',     'pre_loans_max_overdue_sum_3_agg',     'pre_loans_next_pay_summ',     'pre_loans_outstanding',     'pre_loans_outstanding_1',     'pre_loans_outstanding_1_agg',     'pre_loans_outstanding_2',     'pre_loans_outstanding_2_agg',     'pre_loans_outstanding_3',     'pre_loans_outstanding_3_agg',     'pre_loans_outstanding_4',     'pre_loans_outstanding_4_agg',     'pre_loans_outstanding_5',     'pre_loans_outstanding_5_agg',     'pre_loans_total_overdue',\n",
    "         'pre_loans_total_overdue_0_agg',     'pre_loans_total_overdue_1_agg',     'pre_maxover2limit',     'pre_maxover2limit_0_agg',     'pre_maxover2limit_10_agg',     'pre_maxover2limit_11_agg',     'pre_maxover2limit_12_agg',     'pre_maxover2limit_13_agg',     'pre_maxover2limit_14_agg',     'pre_maxover2limit_15_agg',     'pre_maxover2limit_16_agg',     'pre_maxover2limit_17_agg',     'pre_maxover2limit_18_agg',     'pre_maxover2limit_19_agg',     'pre_maxover2limit_1_agg',     'pre_maxover2limit_2_agg',     'pre_maxover2limit_3_agg',     'pre_maxover2limit_4_agg',     'pre_maxover2limit_5_agg',     'pre_maxover2limit_6_agg',     'pre_maxover2limit_7_agg',     'pre_maxover2limit_8_agg',     'pre_maxover2limit_9_agg',     'pre_over2limit',     'pre_over2limit_0_agg',     'pre_over2limit_10_agg',\n",
    "         'pre_over2limit_11_agg',     'pre_over2limit_12_agg',     'pre_over2limit_13_agg',     'pre_over2limit_14_agg',     'pre_over2limit_15_agg',     'pre_over2limit_16_agg',     'pre_over2limit_17_agg',     'pre_over2limit_18_agg',     'pre_over2limit_19_agg',     'pre_over2limit_1_agg',     'pre_over2limit_2_agg',     'pre_over2limit_3_agg',     'pre_over2limit_4_agg',     'pre_over2limit_5_agg',     'pre_over2limit_6_agg',     'pre_over2limit_7_agg',     'pre_over2limit_8_agg',     'pre_over2limit_9_agg',     'pre_pterm',     'pre_since_confirmed',     'pre_since_confirmed_0_agg',     'pre_since_confirmed_10_agg',     'pre_since_confirmed_11_agg',     'pre_since_confirmed_12_agg',     'pre_since_confirmed_13_agg',     'pre_since_confirmed_14_agg',     'pre_since_confirmed_15_agg',     'pre_since_confirmed_16_agg',\n",
    "         'pre_since_confirmed_17_agg',     'pre_since_confirmed_1_agg',     'pre_since_confirmed_2_agg',     'pre_since_confirmed_3_agg',     'pre_since_confirmed_4_agg',     'pre_since_confirmed_5_agg',     'pre_since_confirmed_6_agg',     'pre_since_confirmed_7_agg',     'pre_since_confirmed_8_agg',     'pre_since_confirmed_9_agg',     'pre_since_opened',     'pre_since_opened_0_agg',     'pre_since_opened_10_agg',     'pre_since_opened_11_agg',     'pre_since_opened_12_agg',     'pre_since_opened_13_agg',     'pre_since_opened_14_agg',\n",
    "         'pre_since_opened_15_agg',     'pre_since_opened_16_agg',     'pre_since_opened_17_agg',     'pre_since_opened_18_agg',     'pre_since_opened_19_agg',     'pre_since_opened_1_agg',     'pre_since_opened_2_agg',     'pre_since_opened_3_agg',     'pre_since_opened_4_agg',     'pre_since_opened_5_agg',     'pre_since_opened_6_agg',     'pre_since_opened_7_agg',     'pre_since_opened_8_agg',     'pre_since_opened_9_agg',     'pre_till_fclose',     'pre_till_pclose',     'pre_util',     'pre_util_0_agg',     'pre_util_10_agg',     'pre_util_11_agg',     'pre_util_12_agg',     'pre_util_13_agg',\n",
    "         'pre_util_14_agg',     'pre_util_15_agg',     'pre_util_16_agg',     'pre_util_17_agg',     'pre_util_18_agg',     'pre_util_19_agg',     'pre_util_1_agg',     'pre_util_2_agg',     'pre_util_3_agg',     'pre_util_4_agg',     'pre_util_5_agg',     'pre_util_6_agg',\n",
    "         'pre_util_7_agg',     'pre_util_8_agg',     'pre_util_9_agg',     'prone_to_close_faster',     'prone_to_overdue',\n",
    "         'rn']\n",
    "        with open('all_columns_list.txt', 'w') as file:\n",
    "            for item in all_columns:\n",
    "                file.write(item + '\\n')\n",
    "                \n",
    "        with open('all_columns_list.txt', 'r') as file:\n",
    "            all_columns = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    missing_columns = []\n",
    "    \n",
    "    for col in all_columns:\n",
    "        if col not in df.columns:\n",
    "            missing_columns.append(col)\n",
    "    \n",
    "    for col in missing_columns:\n",
    "        df[col] = 0\n",
    "            \n",
    "    return df.reindex(sorted(df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8a61f",
   "metadata": {},
   "source": [
    "# base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99ff2573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Grad_2:\n",
    "    'This class is made for streamlining the process of data preparation and making predictions with my model of choice'\n",
    "\n",
    "    \n",
    "    def __init__(self, path_to_dataset = 'train_data/', default_transformers = list):\n",
    "        \"\"\"\n",
    "        Initializes Grad_2 \n",
    "\n",
    "        :param path_to_dataset: path to fragmented dataset\n",
    "        :param default_transformers: list of predetermined transformers for data\n",
    "        \"\"\"\n",
    "        self.path_to_dataset = path_to_dataset\n",
    "        self.default_transformers = default_transformers\n",
    "        \n",
    "        \n",
    "    def prepare_dataset(self, path_to_dataset: str, start_from: int = 0,\n",
    "                                         num_parts_to_read: int = 2, columns=None, verbose=True,\n",
    "                                       transformers: list = [],\n",
    "                                       ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Предоставленный инструмент по сбору итогового датасета.\n",
    "        \n",
    "        Доработки:\n",
    "        1) теперь на вход так же принимает лист функций, в дальнейшем они оборачиваются в function_transformer \n",
    "        и служат этапами в пайплайне по обработке данных\n",
    "\n",
    "        2) была заменена библиотека для чтения паркет-файлов - pandas и pyarrow у меня моментально \n",
    "        убивают ядро - вместо этого используется fastparquet - свою функцию выполняет.\n",
    "        \n",
    "        3) в следствии различного наполнения кусков сета данными и особенностей агрегации \"на выходе\" добавлен код, \n",
    "        который заполняет пустоты нулями, чтобы модель не ругалась\n",
    "\n",
    "        читает num_parts_to_read партиций, преобразовывает их к pd.DataFrame и возвращает\n",
    "        :param path_to_dataset: путь до директории с партициями\n",
    "        :param start_from: номер партиции, с которой нужно начать чтение\n",
    "        :param num_parts_to_read: количество партиций, которые требуется прочитать\n",
    "        :param columns: список колонок, которые нужно прочитать из партиции\n",
    "---new! :param transformers: лист функций-трансформеров данных                        \n",
    "        :return: pd.DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        res = []\n",
    "        dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                                  if filename.startswith('train')])\n",
    "        print(dataset_paths)\n",
    "\n",
    "        start_from = max(0, start_from)\n",
    "        chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "        if verbose:\n",
    "            print('Reading chunks:\\n')\n",
    "            for chunk in chunks:\n",
    "                print(chunk)\n",
    "\n",
    "\n",
    "        transformers_trf = []  \n",
    "        for i in range(len(transformers)):  \n",
    "                transformers_trf.append((transformers[i].__name__+'_trf', FunctionTransformer(transformers[i])))  \n",
    "        pipeline = Pipeline(transformers_trf) \n",
    "\n",
    "        for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset\"): \n",
    "            print('chunk_path ', chunk_path)\n",
    "            chunk = fastparquet.ParquetFile(chunk_path).to_pandas()\n",
    "\n",
    "            chunk = pipeline.fit_transform(chunk)  \n",
    "\n",
    "            res.append(chunk)\n",
    "\n",
    "\n",
    "        res = pd.concat(res).reset_index(drop=True)\n",
    "\n",
    "        keys = res.isnull().sum().keys()\n",
    "        nan_values = res.isnull().sum()\n",
    "        cols_w_nans = []\n",
    "        for key in keys:\n",
    "            if nan_values[key] > 0:\n",
    "                cols_w_nans.append(key)\n",
    "\n",
    "        for col in cols_w_nans:\n",
    "            res.loc[res[col].isna() == True, col] = 0\n",
    "\n",
    "\n",
    "\n",
    "        return res             \n",
    "        \n",
    "        \n",
    "    def write_dataset_down(self, df: pd.DataFrame, path_to_dataset: str,):\n",
    "        \"\"\"\"\n",
    "        Writes dataset down in the same folder as its fragmented and non-aggregated part were stored in i.e. '/train_data'\n",
    "        \n",
    "        :param df: dataset to be written down to .pq format\n",
    "        :param path_to_dataset: path to fragmented dataset\n",
    "        :return: None\n",
    "        \n",
    "        ex.:\n",
    "        class_exemplar.write_dataset_down(dataframe, 'train_data')\n",
    "        \"\"\"\n",
    "        tst = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                                          if filename.startswith('df_compiled_v')])\n",
    "        versions = []\n",
    "        for entry in tst:\n",
    "            versions.append(eval(entry.split('v')[1][:entry.split('v')[1].find('_')]))\n",
    "        next_version = max(versions)+1\n",
    "        next_version\n",
    "\n",
    "        path = f'{path_to_dataset}/df_compiled_v{next_version}_{df.shape[1]}_cols_full.pq'\n",
    "        df.to_parquet(path)\n",
    "        pass\n",
    "\n",
    "    def fit_model(self, df: pd.DataFrame, ):\n",
    "        \"\"\"\"\n",
    "        Fits XGBClassifier with prepared data, gives back fiitted model, X_test and y_test\n",
    "        \n",
    "        :param df: dataset for model to be fit with\n",
    "        :return: xgboost.sklearn.XGBClassifier, pd.DataFrame, pd.DataFrame\n",
    "        \n",
    "        ex.:\n",
    "        model, X_test, y_test = class_exemplar.fit_model(dataframe)\n",
    "        \"\"\"\n",
    "\n",
    "        shares = True\n",
    "        share = (60, 40)\n",
    "\n",
    "        if shares == True:\n",
    "            shares = f'{share[0]}-{share[1]} shares'\n",
    "        else:\n",
    "            shares = f'no shares'\n",
    "\n",
    "        df2, df_transformed_test = train_test_split(df, stratify=df['flag'], test_size=0.2, )\n",
    "\n",
    "        df2 = sample_df(df2,round(df2.flag.value_counts()[1] / share[1] * 100-1, 0) , share[0], share[1])\n",
    "\n",
    "        X, y = df2.drop('flag', axis=1), df2.flag\n",
    "        X_test, y_test = df_transformed_test.drop('flag', axis=1), df_transformed_test.flag\n",
    "\n",
    "        model = xgb.XGBClassifier(colsample_bytree=0.9, gamma=0.1, learning_rate=0.1, max_depth=5, n_estimators=400, subsample=0.8)\n",
    "\n",
    "        model.fit(X, y) \n",
    "        \n",
    "        self.fitted_model = model\n",
    "        \n",
    "        return model, X_test, y_test\n",
    "\n",
    "    \n",
    "    def write_down_model(self, model, model_filename: str):\n",
    "        \"\"\"\"\n",
    "        writes down fitted model for later use\n",
    "        \n",
    "        :param model: takes fitted model\n",
    "        :param model_filename: str file name for written down model, without .pkl, just the name\n",
    "        :return: None\n",
    "        \n",
    "        ex.:\n",
    "        class_exemplar.write_down_model(fitted_model, 'model_1')\n",
    "        \"\"\"\n",
    "        if type(model_filename) != str:\n",
    "            return print('wrong filename type!')\n",
    "            \n",
    "        joblib.dump(model, f'models/{model_filename}.pkl')\n",
    "        pass\n",
    "\n",
    "\n",
    "    def load_model(self, path_to_model: str):\n",
    "        \"\"\"\"\n",
    "        Loads model from designated local folder\n",
    "        \n",
    "        :param path_to_model: designated local folder\n",
    "        :return: xgboost.sklearn.XGBClassifier - supposedly fitted model\n",
    "        \n",
    "        ex.:\n",
    "        loaded_model = class_exemplar.load_model('models/model_1.pkl')\n",
    "        \"\"\"\n",
    "        model = joblib.load(path_to_model)\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def predict(self, fitted_model, X_test, y_test, write_it_down = True ):\n",
    "        \"\"\"\"\n",
    "        Makes prediction based on a provided test-chunk of a dataset\n",
    "        \n",
    "        :param fitted_model: fitted model\n",
    "        :param X_test: prediction data\n",
    "        :param y_test: target data\n",
    "        :write_it_down True/False: write down predictions to 'predicts/' folder. or dont\n",
    "        :return: array\n",
    "        \n",
    "        ex.:\n",
    "        prediced_probabilities = class_exemplar.predict(fitted_model, X_test, y_test, True):\n",
    "        \"\"\"\n",
    "        y_pred_proba = fitted_model.predict_proba(X_test)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "        if write_it_down:\n",
    "            with open('predicts/predictions.txt', 'w') as file:\n",
    "                for item in y_pred_proba[:, 1]:\n",
    "                    file.write(str(item) + '\\n')\n",
    "            \n",
    "        \n",
    "        return y_pred_proba[:, 1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit(self, \n",
    "            path_to_dataset: str, \n",
    "            start_from: int = 0,\n",
    "            num_parts_to_read: int = 2,\n",
    "            columns=None, \n",
    "            verbose=True,                                       \n",
    "            transformers: list = [],\n",
    "           ) -> pd.DataFrame:\n",
    "        \"\"\"\"\n",
    "        Combines:\n",
    "        Grad_2.prepare_dataset method\n",
    "        Grad_2.fit_model method\n",
    "        \n",
    "        basically preprocesses dataset and fits model in one go\n",
    "        \n",
    "        :param path_to_dataset: путь до директории с партициями\n",
    "        :param start_from: номер партиции, с которой нужно начать чтение\n",
    "        :param num_parts_to_read: количество партиций, которые требуется прочитать\n",
    "        :param columns: список колонок, которые нужно прочитать из партиции\n",
    "---new! :param transformers: лист функций-трансформеров данных                        \n",
    "        :return:    xgboost.sklearn.XGBClassifier - fitted model,\n",
    "                    pd.DataFrame - X_test prediction data, \n",
    "                    pd.DataFrame - y_test target array\n",
    "        \"\"\"\n",
    "        res = self.prepare_dataset(path_to_dataset,\n",
    "                                   start_from,\n",
    "                                   num_parts_to_read, \n",
    "                                   columns, \n",
    "                                   verbose,\n",
    "                                   transformers,\n",
    "                                  )\n",
    "        \n",
    "        model, X_test, y_test = self.fit_model(res)\n",
    "        \n",
    "        return model, X_test, y_test\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf7df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4049120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
